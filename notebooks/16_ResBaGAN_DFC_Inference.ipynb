{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7859e55f",
   "metadata": {},
   "source": [
    "# Fase 1.6: Inferencia con la CNN utilizando el acelerador Hailo 8L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfce5ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "from multiprocessing import Process\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from hailo_platform import (\n",
    "    HEF,\n",
    "    ConfigureParams,\n",
    "    FormatType,\n",
    "    HailoSchedulingAlgorithm,\n",
    "    HailoStreamInterface,\n",
    "    InferVStreams,\n",
    "    InputVStreamParams,\n",
    "    InputVStreams,\n",
    "    OutputVStreamParams,\n",
    "    OutputVStreams,\n",
    "    VDevice,\n",
    ")\n",
    "\n",
    "# from hailo_sdk_client import ClientRunner, InferenceContext\n",
    "\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Funciones y parámetros de la CNN base\n",
    "import sys\n",
    "sys.path.append('../src/')\n",
    "import resbagan_networks\n",
    "import resbagan_datasets\n",
    "from cnn21_pix import read_pgm, save_pgm\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac99d1c",
   "metadata": {},
   "source": [
    "## 1. Inferencia mediante flujo independiente"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c20a1d",
   "metadata": {},
   "source": [
    "### 1.1. Definición de parámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ded78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting VDevice params to disable the HailoRT service feature\n",
    "params = VDevice.create_params()\n",
    "params.scheduling_algorithm = HailoSchedulingAlgorithm.NONE\n",
    "\n",
    "# The target can be used as a context manager (\"with\" statement) to ensure it's released on time.\n",
    "# Here it's avoided for the sake of simplicity\n",
    "target = VDevice(params=params)\n",
    "\n",
    "# Loading compiled HEFs to device:\n",
    "optimization_level = 1\n",
    "compression_level = 0\n",
    "model_name = f\"../results/models/model_ResBaGAN_discriminator_o{optimization_level}_c{compression_level}\"\n",
    "hef_path = f\"{model_name}.hef\"\n",
    "hef = HEF(hef_path)\n",
    "\n",
    "# Get the \"network groups\" (connectivity groups, aka. \"different networks\") information from the .hef\n",
    "configure_params = ConfigureParams.create_from_hef(hef=hef, interface=HailoStreamInterface.PCIe)\n",
    "network_groups = target.configure(hef, configure_params)\n",
    "network_group = network_groups[0]\n",
    "network_group_params = network_group.create_params()\n",
    "\n",
    "# Create input and output virtual streams params\n",
    "# Quantized argument signifies whether or not the incoming data is already quantized.\n",
    "# Data is quantized by HailoRT if and only if quantized == False .\n",
    "input_vstreams_params = InputVStreamParams.make(network_group, quantized=False, format_type=FormatType.FLOAT32)\n",
    "output_vstreams_params = OutputVStreamParams.make(network_group, quantized=True, format_type=FormatType.UINT8)\n",
    "\n",
    "# Obtener información de entrada/salida\n",
    "input_vstream_info = hef.get_input_vstream_infos()[0]\n",
    "output_vstream_info = hef.get_output_vstream_infos()[0]\n",
    "image_height, image_width, num_bands = input_vstream_info.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fea501a",
   "metadata": {},
   "source": [
    "### 1.2. Carga del dataset y ejecución de la inferencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37daac95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el dataset y ejecutar la inferencia\n",
    "\n",
    "# Se mide el tiempo total de carga y ejecución\n",
    "start_time = time.time()\n",
    "\n",
    "# Definir parámetros\n",
    "\n",
    "DATASET='../data/imagenes_rios/oitaven_river.raw'\n",
    "GT='../data/imagenes_rios/oitaven_river.pgm'\n",
    "\n",
    "# Queremos usar todos los datos para la inferencia\n",
    "SAMPLES=[0.0,0.0]\n",
    "\n",
    "os.environ['HAILO_MONITOR'] = '1'\n",
    "\n",
    "# Carga de datos para la inferencia en el discriminador\n",
    "dataset = resbagan_datasets.HyperDataset(\n",
    "    \"oitaven_river\", segmented=False, patch_size=32, ratios=(SAMPLES[0], SAMPLES[1]))\n",
    "\n",
    "# Almacenamos las dimensiones en variables\n",
    "H = dataset.height\n",
    "V = dataset.width\n",
    "\n",
    "samples = dataset.ordered_test_set['samples']\n",
    "\n",
    "# Obtenemos el array de gt\n",
    "truth = dataset.gt.flatten()\n",
    "# Obtenemos un array de indices para test\n",
    "test = dataset.test_index_list\n",
    "\n",
    "# Obtenemos el numero de clases y el numero de clases no vacias para test\n",
    "nclases = dataset.classes_count\n",
    "nclases_no_vacias = 0\n",
    "for i in range(nclases):\n",
    "    clase_actual = i + 1\n",
    "    if any(truth[idx] == clase_actual for idx in test):\n",
    "        nclases_no_vacias += 1\n",
    "\n",
    "print(dataset)\n",
    "\n",
    "# The custom HyperDataset object contains all the train, validation and test data\n",
    "#   --> But it will wrapped into a PyTorch data feeder for convenience\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=100,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    pin_memory=False,\n",
    ")\n",
    "\n",
    "# Ejecutar la inferencia\n",
    "\n",
    "output=np.zeros(H*V, dtype=np.uint8)\n",
    "\n",
    "# Modo evaluación\n",
    "dataset.to_test()\n",
    "\n",
    "# Realizar la predicción\n",
    "with InferVStreams(network_group, input_vstreams_params, output_vstreams_params) as infer_pipeline:\n",
    "    with network_group.activate(network_group_params):\n",
    "        total=0\n",
    "        for batch_id, (inputs, labels, targets_pixel_level) in enumerate(data_loader):\n",
    "            \n",
    "            # Convertir inputs a un formato adecuado para hailo\n",
    "            inputs_np = inputs.numpy()\n",
    "            inputs_hailo = np.transpose(inputs_np, (0, 2, 3, 1))\n",
    "\n",
    "            # Asegurarse de que los datos sean contiguos en memoria\n",
    "            inputs_hailo = np.ascontiguousarray(inputs_hailo.astype(np.float32))\n",
    "            \n",
    "            # Realizar la inferencia\n",
    "            input_data = {input_vstream_info.name: inputs_hailo}\n",
    "            \n",
    "            infer_results = infer_pipeline.infer(input_data)\n",
    "            output_hailo = infer_results[output_vstream_info.name]\n",
    "            output_hailo_formatted = np.delete(output_hailo, dataset.classes_count, axis=-1) # Se deshabilita la clase fake para test\n",
    "\n",
    "            predicted=np.argmax(output_hailo_formatted, axis=-1)\n",
    "\n",
    "            for i in range(len(predicted)):\n",
    "                output[test[total+i]]=np.uint8(predicted[i]+1)\n",
    "            total+=labels.size(0)\n",
    "            if(total%100000==0): print('  Test:',total,'/', len(dataset))\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(\"Prediction time: {:.4f} seconds\".format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe118ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar el output\n",
    "\n",
    "np.save(f'../results/predictions/predictions_rpi_ResBaGAN_discriminator_o{optimization_level}_c{compression_level}.npy', output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd7ea9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el output\n",
    "\n",
    "output = np.load(f'../results/predictions/predictions_rpi_ResBaGAN_discriminator_o{optimization_level}_c{compression_level}.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11df7f72",
   "metadata": {},
   "source": [
    "### 1.3. Evaluación del desempeño del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838e121e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluar el desempeño del modelo\n",
    "\n",
    "# Precisiones a nivel de clase\n",
    "correct=0; total=0; AA=0; OA=0\n",
    "class_correct=[0]*(nclases+1)\n",
    "class_total=[0]*(nclases+1)\n",
    "class_aa=[0]*(nclases+1)\n",
    "\n",
    "for i in test:\n",
    "    if(output[i]==0 or truth[i]==0): continue\n",
    "    total+=1; class_total[truth[i]]+=1\n",
    "    if(output[i]==truth[i]):\n",
    "          correct+=1\n",
    "          class_correct[truth[i]]+=1\n",
    "for i in range(1,nclases+1):\n",
    "    if(class_total[i]!=0): class_aa[i]=100*class_correct[i]/class_total[i]\n",
    "    else: class_aa[i]=0\n",
    "    AA+=class_aa[i]\n",
    "OA=100*correct/total; AA=AA/nclases_no_vacias\n",
    "\n",
    "for i in range(1,nclases+1): print('  Class %02d: %02.02f'%(i,class_aa[i]))\n",
    "print('* Accuracy (pixels) OA=%02.02f, AA=%02.02f'%(OA,AA))\n",
    "print('  total:',total,'correct:',correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881d4a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar la salida\n",
    "\n",
    "save_pgm(output,V,H,nclases,f'../results/predictions/predictions_rpi_ResBaGAN_discriminator_o{optimization_level}_c{compression_level}.pgm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b228602",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostrar la salida\n",
    "\n",
    "OUTPUT=f'../results/predictions/predictions_rpi_ResBaGAN_discriminator_o{optimization_level}_c{compression_level}.pgm'\n",
    "\n",
    "(imagen_output, H, V) = read_pgm(OUTPUT)\n",
    "\n",
    "# Convertir la lista a array y redimensionar\n",
    "imagen_output = np.array(imagen_output, dtype=np.uint8).reshape(V, H)\n",
    "\n",
    "# Mostrar la imagen\n",
    "plt.imshow(imagen_output)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647d6e5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d19a89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e13d86d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (hailo_rpi_env)",
   "language": "python",
   "name": "hailo_rpi_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
