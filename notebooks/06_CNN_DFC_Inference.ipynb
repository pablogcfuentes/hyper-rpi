{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7859e55f",
   "metadata": {},
   "source": [
    "# Fase 0.6: Inferencia con la CNN utilizando el acelerador Hailo 8L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfce5ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "from multiprocessing import Process\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from hailo_platform import (\n",
    "    HEF,\n",
    "    ConfigureParams,\n",
    "    FormatType,\n",
    "    HailoSchedulingAlgorithm,\n",
    "    HailoStreamInterface,\n",
    "    InferVStreams,\n",
    "    InputVStreamParams,\n",
    "    InputVStreams,\n",
    "    OutputVStreamParams,\n",
    "    OutputVStreams,\n",
    "    VDevice,\n",
    ")\n",
    "\n",
    "# from hailo_sdk_client import ClientRunner, InferenceContext\n",
    "\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Funciones y parámetros de la CNN base\n",
    "import sys\n",
    "sys.path.append('../src/')\n",
    "\n",
    "from cnn21_pix import *\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac99d1c",
   "metadata": {},
   "source": [
    "## 1. Inferencia mediante flujo independiente"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c20a1d",
   "metadata": {},
   "source": [
    "### 1.1. Definición de parámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ded78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting VDevice params to disable the HailoRT service feature\n",
    "params = VDevice.create_params()\n",
    "params.scheduling_algorithm = HailoSchedulingAlgorithm.NONE\n",
    "\n",
    "# The target can be used as a context manager (\"with\" statement) to ensure it's released on time.\n",
    "# Here it's avoided for the sake of simplicity\n",
    "target = VDevice(params=params)\n",
    "\n",
    "# Loading compiled HEFs to device:\n",
    "optimization_level = 0\n",
    "compression_level = 0\n",
    "model_name = f\"../results/models/model_cnn21_o{optimization_level}_c{compression_level}\"\n",
    "hef_path = f\"{model_name}.hef\"\n",
    "hef = HEF(hef_path)\n",
    "\n",
    "# Get the \"network groups\" (connectivity groups, aka. \"different networks\") information from the .hef\n",
    "configure_params = ConfigureParams.create_from_hef(hef=hef, interface=HailoStreamInterface.PCIe)\n",
    "network_groups = target.configure(hef, configure_params)\n",
    "network_group = network_groups[0]\n",
    "network_group_params = network_group.create_params()\n",
    "\n",
    "# Create input and output virtual streams params\n",
    "# Quantized argument signifies whether or not the incoming data is already quantized.\n",
    "# Data is quantized by HailoRT if and only if quantized == False .\n",
    "input_vstreams_params = InputVStreamParams.make(network_group, quantized=False, format_type=FormatType.FLOAT32)\n",
    "output_vstreams_params = OutputVStreamParams.make(network_group, quantized=True, format_type=FormatType.UINT8)\n",
    "\n",
    "# Obtener información de entrada/salida\n",
    "input_vstream_info = hef.get_input_vstream_infos()[0]\n",
    "output_vstream_info = hef.get_output_vstream_infos()[0]\n",
    "image_height, image_width, num_bands = input_vstream_info.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fea501a",
   "metadata": {},
   "source": [
    "### 1.2. Carga del dataset y ejecución de la inferencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d072c0af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Cargar el dataset y ejecutar la inferencia\n",
    "\n",
    "# Se mide el tiempo total de carga y ejecución\n",
    "start_time = time.time()\n",
    "\n",
    "# Definir parámetros\n",
    "\n",
    "DATASET='../data/imagenes_rios/oitaven_river.raw'\n",
    "GT='../data/imagenes_rios/oitaven_river.pgm'\n",
    "\n",
    "# Queremos usar todos los datos para la inferencia\n",
    "SAMPLES=[0,0]\n",
    "PAD=1\n",
    "AUM=0\n",
    "\n",
    "os.environ['HAILO_MONITOR'] = '1'\n",
    "\n",
    "# Carga de datos\n",
    "\n",
    "(datos,H,V,B)=read_raw(DATASET)\n",
    "(truth,H1,V1)=read_pgm(GT)\n",
    "\n",
    "# Durante la ejecucion de la red vamos a coger patches de tamano cuadrado\n",
    "sizex=32; sizey=32 \n",
    "\n",
    "# Hacemos padding en el dataset para poder aprovechar hasta el borde\n",
    "if(PAD):\n",
    "    datos=torch.FloatTensor(np.pad(datos,((sizey//2,sizey//2),(sizex//2,sizex//2),(0,0)),'symmetric'))\n",
    "    H=H+2*(sizex//2); V=V+2*(sizey//2)\n",
    "    truth=np.reshape(truth,(-1,H1))\n",
    "    truth=np.pad(truth,((sizey//2,sizey//2),(sizex//2,sizex//2)),'constant')\n",
    "    H1=H1+2*(sizex//2); V1=V1+2*(sizey//2)\n",
    "    truth=np.reshape(truth,(H1*V1))\n",
    "    \n",
    "# Necesitamos los datos en band-vector para hacer convoluciones\n",
    "datos=np.transpose(datos,(2,0,1))\n",
    "\n",
    "# Seleccionar conjunto de test (en este caso es una predicción)\n",
    "(train,val,test,nclases,nclases_no_vacias)=select_training_samples(truth,H,V,sizex,sizey,SAMPLES)\n",
    "dataset_test=HyperDataset(datos,truth,test,H,V,sizex,sizey)\n",
    "print('  - test dataset:',len(dataset_test))\n",
    "\n",
    "# Dataloader\n",
    "batch_size=100 # defecto 100\n",
    "\n",
    "test_loader=DataLoader(dataset_test,batch_size,shuffle=False)\n",
    "\n",
    "# Ejecutar la inferencia\n",
    "\n",
    "output=np.zeros(H*V,dtype=np.uint8)\n",
    "\n",
    "with InferVStreams(network_group, input_vstreams_params, output_vstreams_params) as infer_pipeline:\n",
    "    with network_group.activate(network_group_params):\n",
    "        total = 0\n",
    "        \n",
    "        for (inputs, labels) in test_loader:\n",
    "            # Convertir inputs a un formato adecuado para hailo\n",
    "            inputs_np = inputs.numpy()\n",
    "            inputs_hailo = np.transpose(inputs_np, (0, 2, 3, 1))\n",
    "\n",
    "            # Asegurarse de que los datos sean contiguos en memoria\n",
    "            inputs_hailo = np.ascontiguousarray(inputs_hailo.astype(np.float32))\n",
    "            \n",
    "            # Realizar la inferencia\n",
    "            input_data = {input_vstream_info.name: inputs_hailo}\n",
    "            \n",
    "            infer_results = infer_pipeline.infer(input_data)\n",
    "\n",
    "            predicted=np.argmax(infer_results[output_vstream_info.name], axis=1)\n",
    "\n",
    "            # Asignar las predicciones al array de salida\n",
    "            for i in range(len(predicted)):\n",
    "                output[test[total+i]]=np.uint8(predicted[i]+1)\n",
    "            total+=labels.size(0)\n",
    "\n",
    "            # Mostrar el progreso\n",
    "            if(total%100000==0): print('  Test:',total,'/',len(dataset_test))\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(\"Prediction time: {:.4f} seconds\".format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c2c3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar el output\n",
    "\n",
    "np.save(f'../results/predictions/predictions_rpi_cnn21_o{optimization_level}_c{compression_level}.npy', output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f20525",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el output\n",
    "\n",
    "output = np.load(f'../results/predictions/predictions_rpi_cnn21_o{optimization_level}_c{compression_level}.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11df7f72",
   "metadata": {},
   "source": [
    "### 1.3. Evaluación del desempeño del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838e121e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluar el desempeño del modelo\n",
    "\n",
    "# Precisiones a nivel de clase\n",
    "correct=0; total=0; AA=0; OA=0\n",
    "class_correct=[0]*(nclases+1)\n",
    "class_total=[0]*(nclases+1)\n",
    "class_aa=[0]*(nclases+1)\n",
    "\n",
    "for i in test:\n",
    "    if(output[i]==0 or truth[i]==0): continue\n",
    "    total+=1; class_total[truth[i]]+=1\n",
    "    if(output[i]==truth[i]):\n",
    "          correct+=1\n",
    "          class_correct[truth[i]]+=1\n",
    "for i in range(1,nclases+1):\n",
    "    if(class_total[i]!=0): class_aa[i]=100*class_correct[i]/class_total[i]\n",
    "    else: class_aa[i]=0\n",
    "    AA+=class_aa[i]\n",
    "OA=100*correct/total; AA=AA/nclases_no_vacias\n",
    "\n",
    "for i in range(1,nclases+1): print('  Class %02d: %02.02f'%(i,class_aa[i]))\n",
    "print('* Accuracy (pixels) OA=%02.02f, AA=%02.02f'%(OA,AA))\n",
    "print('  total:',total,'correct:',correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881d4a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar la salida\n",
    "\n",
    "if(PAD):\n",
    "    output=np.reshape(output,(-1,H1))\n",
    "    output=output[sizey//2:V1-sizey//2,sizex//2:H1-sizex//2]\n",
    "    H1=H1-2*(sizex//2); V1=V1-2*(sizey//2)\n",
    "    output=np.reshape(output,(H1*V1))\n",
    "\n",
    "save_pgm(output,H1,V1,nclases,f'../results/predictions/predictions_rpi_cnn21_o{optimization_level}_c{compression_level}.pgm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b228602",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostrar la salida\n",
    "\n",
    "OUTPUT=f'../results/predictions/predictions_rpi_cnn21_o{optimization_level}_c{compression_level}.pgm'\n",
    "\n",
    "(imagen_output, H1, V1) = read_pgm(OUTPUT)\n",
    "\n",
    "# Convertir la lista a array y redimensionar\n",
    "imagen_output = np.array(imagen_output, dtype=np.uint8).reshape(V1, H1)\n",
    "\n",
    "# Mostrar la imagen\n",
    "plt.imshow(imagen_output)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647d6e5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d19a89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5adfc9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (hailo_rpi_env)",
   "language": "python",
   "name": "hailo_rpi_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
