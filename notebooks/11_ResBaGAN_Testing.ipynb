{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d5e60a4",
   "metadata": {},
   "source": [
    "# Fase 1.1: Evaluar una ResBaGAN para clasificación de imágenes hiperespectrales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fef7d0d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import math,random,struct,os,time,sys\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.onnx\n",
    "from torchvision import models\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from sklearn import preprocessing\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "# Funciones y parámetros de la ResBaGAN\n",
    "import sys\n",
    "sys.path.append('../src/')\n",
    "\n",
    "import resbagan_networks\n",
    "import resbagan_datasets\n",
    "from cnn21_pix import read_pgm, save_pgm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46aafd26",
   "metadata": {},
   "source": [
    "## 1. Uso del flujo general para la ResBaGAN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03251c79",
   "metadata": {},
   "source": [
    "### 1.1. Obtención del discriminador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3591bad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir parámetros\n",
    "\n",
    "DATASET='../data/imagenes_rios/oitaven_river.raw'\n",
    "GT='../data/imagenes_rios/oitaven_river.pgm'\n",
    "MODEL=\"../results/models/model_ResBaGAN.pt\"\n",
    "\n",
    "# Queremos usar todos los datos para la inferencia\n",
    "SAMPLES=[0.0,0.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6101d324",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carga de datos para la inferencia en el discriminador\n",
    "\n",
    "dataset = resbagan_datasets.HyperDataset(\n",
    "    \"oitaven_river\", segmented=False, patch_size=32, ratios=(SAMPLES[0], SAMPLES[1]))\n",
    "\n",
    "# Almacenamos las dimensiones en variables\n",
    "H = dataset.height\n",
    "V = dataset.width\n",
    "\n",
    "samples = dataset.ordered_test_set['samples']\n",
    "\n",
    "# Obtenemos el array de gt\n",
    "truth = dataset.gt.flatten()\n",
    "# Obtenemos un array de indices para test\n",
    "test = dataset.test_index_list\n",
    "\n",
    "# Obtenemos el numero de clases y el numero de clases no vacias para test\n",
    "nclases = dataset.classes_count\n",
    "nclases_no_vacias = 0\n",
    "for i in range(nclases):\n",
    "    clase_actual = i + 1\n",
    "    if any(truth[idx] == clase_actual for idx in test):\n",
    "        nclases_no_vacias += 1\n",
    "\n",
    "print(dataset)\n",
    "\n",
    "# The custom HyperDataset object contains all the train, validation and test data\n",
    "#   --> But it will wrapped into a PyTorch data feeder for convenience\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=100,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    pin_memory=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070cba6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar la ResBaGAN\n",
    "\n",
    "cuda=True if torch.cuda.is_available() else False\n",
    "device=torch.device('cuda' if cuda else 'cpu')\n",
    "device='cpu'\n",
    "\n",
    "if torch.backends.cudnn.is_available():\n",
    "    print('* Activando CUDNN')\n",
    "    torch.backends.cudnn.enabled=True\n",
    "    torch.backends.cudnn.beBhmark=True\n",
    "\n",
    "hyperparams = {\n",
    "    \"latent_size\":   128,\n",
    "    \"activation\":    \"lrelu\",\n",
    "    \"p_dropout\":     0.05,\n",
    "    \"weight_init\":   \"xavier\",\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"epochs\":        100,\n",
    "    \"batch_size\":    100,\n",
    "    \"num_workers\":   4,\n",
    "    \"device\":        \"cpu\",\n",
    "}\n",
    "\n",
    "network = resbagan_networks.ResBaGAN(dataset=dataset, device=hyperparams[\"device\"], hyperparams=hyperparams)\n",
    "network.load_state_dict(torch.load(MODEL, map_location=hyperparams[\"device\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37039b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separar el discriminador de la ResBaGAN y guardarlo\n",
    "\n",
    "network.discriminator.eval()\n",
    "discriminator = network.discriminator\n",
    "\n",
    "# Guarda solo el discriminador\n",
    "torch.save(discriminator.state_dict(), \"../results/models/model_ResBaGAN_discriminator.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62734ef",
   "metadata": {},
   "source": [
    "### 1.2. Uso del discriminador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5af1286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el discriminador\n",
    "\n",
    "model = resbagan_networks.ResBaGAN_Discriminator(dataset=dataset, device=hyperparams[\"device\"], hyperparams=hyperparams)\n",
    "\n",
    "# Cargar los pesos guardados\n",
    "model.load_state_dict(torch.load(\"../results/models/model_ResBaGAN_discriminator.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbda3fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carga de datos e inferencia\n",
    "\n",
    "# Medir el tiempo de ejecución\n",
    "start_time = time.time()\n",
    "\n",
    "# Definir parámetros\n",
    "\n",
    "DATASET='../data/imagenes_rios/oitaven_river.raw'\n",
    "GT='../data/imagenes_rios/oitaven_river.pgm'\n",
    "MODEL=\"../results/models/model_ResBaGAN.pt\"\n",
    "\n",
    "# Queremos usar todos los datos para la inferencia\n",
    "SAMPLES=[0.0,0.0]\n",
    "\n",
    "# Carga de datos para la inferencia en el discriminador\n",
    "\n",
    "dataset = resbagan_datasets.HyperDataset(\n",
    "    \"oitaven_river\", segmented=False, patch_size=32, ratios=(SAMPLES[0], SAMPLES[1]))\n",
    "\n",
    "# Almacenamos las dimensiones en variables\n",
    "H = dataset.height\n",
    "V = dataset.width\n",
    "\n",
    "samples = dataset.ordered_test_set['samples']\n",
    "\n",
    "# Obtenemos el array de gt\n",
    "truth = dataset.gt.flatten()\n",
    "# Obtenemos un array de indices para test\n",
    "test = dataset.test_index_list\n",
    "\n",
    "# Obtenemos el numero de clases y el numero de clases no vacias para test\n",
    "nclases = dataset.classes_count\n",
    "nclases_no_vacias = 0\n",
    "for i in range(nclases):\n",
    "    clase_actual = i + 1\n",
    "    if any(truth[idx] == clase_actual for idx in test):\n",
    "        nclases_no_vacias += 1\n",
    "\n",
    "print(dataset)\n",
    "\n",
    "# The custom HyperDataset object contains all the train, validation and test data\n",
    "#   --> But it will wrapped into a PyTorch data feeder for convenience\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=100,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    pin_memory=False,\n",
    ")\n",
    "\n",
    "# Inferencia\n",
    "output=np.zeros(H*V, dtype=np.uint8)\n",
    "\n",
    "# Modo evaluación\n",
    "model.eval()\n",
    "dataset.to_test()\n",
    "\n",
    "# Realizar la predicción\n",
    "with torch.no_grad():\n",
    "    total=0\n",
    "    for batch_id, (inputs, labels, targets_pixel_level) in enumerate(data_loader):\n",
    "        inputs=inputs.to(device)\n",
    "        outputs_discriminator, _ = model(inputs)\n",
    "        outputs_discriminator[:, dataset.classes_count] = -math.inf # Se deshabilita la clase fake para test\n",
    "        _, outputs_discriminator = torch.max(outputs_discriminator, dim=1)\n",
    "        predicted_cpu=outputs_discriminator.cpu()\n",
    "        for i in range(len(predicted_cpu)):\n",
    "            output[test[total+i]]=np.uint8(predicted_cpu[i]+1)\n",
    "        total+=labels.size(0)\n",
    "        if(total%100000==0): print('  Test:',total,'/', len(dataset))\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(\"Prediction time: {:.4f} seconds\".format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0708a796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar el output\n",
    "\n",
    "np.save('../results/predictions/predictions_ResBaGAN_discriminator.npy', output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e53525a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el output\n",
    "\n",
    "output = np.load('../results/predictions/predictions_ResBaGAN_discriminator.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c30d53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluar el desempeño del modelo\n",
    "\n",
    "# Precisiones a nivel de clase\n",
    "correct=0; total=0; AA=0; OA=0\n",
    "class_correct=[0]*(nclases+1)\n",
    "class_total=[0]*(nclases+1)\n",
    "class_aa=[0]*(nclases+1)\n",
    "\n",
    "for i in test:\n",
    "    if(output[i]==0 or truth[i]==0): continue\n",
    "    total+=1; class_total[truth[i]]+=1\n",
    "    if(output[i]==truth[i]):\n",
    "          correct+=1\n",
    "          class_correct[truth[i]]+=1\n",
    "for i in range(1,nclases+1):\n",
    "    if(class_total[i]!=0): class_aa[i]=100*class_correct[i]/class_total[i]\n",
    "    else: class_aa[i]=0\n",
    "    AA+=class_aa[i]\n",
    "OA=100*correct/total; AA=AA/nclases_no_vacias\n",
    "\n",
    "for i in range(1,nclases+1): print('  Class %02d: %02.02f'%(i,class_aa[i]))\n",
    "print('* Accuracy (pixels) OA=%02.02f, AA=%02.02f'%(OA,AA))\n",
    "print('  total:',total,'correct:',correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d918dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar la salida\n",
    "\n",
    "save_pgm(output,V,H,nclases,'../results/predictions/predictions_ResBaGAN_discriminator.pgm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925b5b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostrar la salida\n",
    "\n",
    "OUTPUT='../results/predictions/predictions_ResBaGAN_discriminator.pgm'\n",
    "\n",
    "(imagen_output, H, V) = read_pgm(OUTPUT)\n",
    "\n",
    "# Convertir la lista a array y redimensionar\n",
    "imagen_output = np.array(imagen_output, dtype=np.uint8).reshape(V, H)\n",
    "\n",
    "# Mostrar la imagen\n",
    "plt.imshow(imagen_output)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4c119c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7d2983",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa90dbb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:hyper_env] *",
   "language": "python",
   "name": "conda-env-hyper_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
